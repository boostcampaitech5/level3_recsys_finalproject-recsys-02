{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "from typing import Union, Tuple, List\n",
    "from transformers import BertConfig, BertForPreTraining, BertTokenizerFast\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "# from IPython.display import Image\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import normal_\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import re\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import os\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.utils import logging\n",
    "logger = logging.get_logger(__name__)\n",
    "from filelock import FileLock\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('/opt/ml/wine/data/review_df_total.csv',encoding = 'utf-8-sig').loc[:,['user_url','rating','text','wine_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/opt/ml/wine/code/data/feature_map/item2idx.json','r') as f:\n",
    "    item2idx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = review_df[review_df['text'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df['text'] = review_df['text'].apply(lambda x: x + '.' if x[-1] != '.' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_reviews = review_df.groupby('wine_url')['text'].agg(', '.join).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_reviews['wine_id'] = merged_reviews['wine_url'].map(item2idx)\n",
    "merged_reviews = merged_reviews[merged_reviews['wine_id'].isna()==False]\n",
    "merged_reviews['wine_id'] = merged_reviews['wine_id'].astype('int').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_english_and_digits(text):\n",
    "    # Remove any characters that are not English alphabets, digits, periods, or commas at the end of sentences\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9\\s.,]', '', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_reviews['text'] = merged_reviews['text'].apply(keep_english_and_digits)\n",
    "merged_reviews = merged_reviews.sort_values(by = 'wine_id')\n",
    "merged_reviews.to_csv('/opt/ml/wine/data/merged_review.csv',encoding='utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_reviews = pd.read_csv('/opt/ml/wine/data/merged_review.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_text(data):\n",
    "    return '\\n'.join(data)\n",
    "\n",
    "def parallel(func, input,  num_cpu):\n",
    "\n",
    "    chunks = np.array_split(input, num_cpu)\n",
    "\n",
    "    print('Parallelizing with ' +str(num_cpu)+'cores')\n",
    "    with Parallel(n_jobs = num_cpu, backend=\"multiprocessing\") as parallel:\n",
    "        results = parallel(delayed(func)(chunks[i]) for i in range(num_cpu))\n",
    "\n",
    "    for i,data in enumerate(results):\n",
    "        if i == 0:\n",
    "            output = data\n",
    "        else:\n",
    "            output += data\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallelizing with 8cores\n"
     ]
    }
   ],
   "source": [
    "text_data = parallel(merge_text, merged_reviews['text'],  8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing: 100%|████████████████████████████████| 837M/837M [14:28<00:00, 963kB/s]\n"
     ]
    }
   ],
   "source": [
    "file_path = '/opt/ml/wine/data/text_data.txt'\n",
    "from tqdm import tqdm\n",
    "with open(file_path, 'w') as file:\n",
    "    # Set the total file size for the progress bar\n",
    "    total_size = len(text_data)\n",
    "\n",
    "    # Create a tqdm progress bar\n",
    "    with tqdm(total=total_size, unit='B', unit_scale=True, ncols=80, desc=\"Writing\") as pbar:\n",
    "        # Write the text data to the file in chunks\n",
    "        for chunk in text_data:\n",
    "            file.write(chunk)\n",
    "            pbar.update(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "total_lines = len(text_data)\n",
    "with tqdm(total=total_lines, ncols=80, desc=\"Processing\") as pbar:\n",
    "    for text in text_data:\n",
    "        text = text.strip()\n",
    "        if text:\n",
    "            dataset.append(text)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Step 2: Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForPreTraining\n",
    "\n",
    "config = BertConfig(    \n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=8,    # layer num\n",
    "    num_attention_heads=8,    # transformer attention head number\n",
    "    intermediate_size=3072,   # transformer 내에 있는 feed-forward network의 dimension size\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=128,    # embedding size 최대 몇 token까지 input으로 사용할 것인지 지정\n",
    "    pad_token_id=0,\n",
    "    position_embedding_type=\"absolute\"\n",
    ")\n",
    "\n",
    "model = BertForPreTraining(config=config)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "class TextDatasetForNextSentencePrediction(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        file_path: str,\n",
    "        block_size: int,\n",
    "        overwrite_cache=False,\n",
    "        short_seq_probability=0.1,\n",
    "        nsp_probability=0.5,\n",
    "    ):\n",
    "        # 여기 부분은 학습 데이터를 caching하는 부분입니다 :-)\n",
    "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
    "\n",
    "        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=True)\n",
    "        self.short_seq_probability = short_seq_probability\n",
    "        self.nsp_probability = nsp_probability\n",
    "\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            directory,\n",
    "            \"cached_nsp_{}_{}_{}\".format(\n",
    "                tokenizer.__class__.__name__,\n",
    "                str(block_size),\n",
    "                filename,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "\n",
    "        # Input file format:\n",
    "        # (1) One sentence per line. These should ideally be actual sentences, not\n",
    "        # entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "        # sentence boundaries for the \"next sentence prediction\" task).\n",
    "        # (2) Blank lines between documents. Document boundaries are needed so\n",
    "        # that the \"next sentence prediction\" task doesn't span between documents.\n",
    "        #\n",
    "        # Example:\n",
    "        # I am very happy.\n",
    "        # Here is the second sentence.\n",
    "        #\n",
    "        # A new document.\n",
    "\n",
    "        with FileLock(lock_path):\n",
    "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    self.examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
    "                # 여기서부터 본격적으로 dataset을 만듭니다.\n",
    "                self.documents = [[]]\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    while True: # 일단 문장을 읽고\n",
    "                        line = f.readline()\n",
    "                        if not line:\n",
    "                            break\n",
    "                        line = line.strip()\n",
    "\n",
    "                        # 이중 띄어쓰기가 발견된다면, 나왔던 문장들을 모아 하나의 문서로 묶어버립니다.\n",
    "                        # 즉, 문단 단위로 데이터를 저장합니다.\n",
    "                        if not line and len(self.documents[-1]) != 0:\n",
    "                            self.documents.append([])\n",
    "                        tokens = tokenizer.tokenize(line)\n",
    "                        tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                        if tokens:\n",
    "                            self.documents[-1].append(tokens)\n",
    "                # 이제 코퍼스 전체를 읽고, 문서 데이터를 생성했습니다! :-)\n",
    "                logger.info(f\"Creating examples from {len(self.documents)} documents.\")\n",
    "                self.examples = []\n",
    "                # 본격적으로 학습을 위한 데이터로 변형시켜볼까요?\n",
    "                for doc_index, document in enumerate(self.documents):\n",
    "                    self.create_examples_from_document(document, doc_index) # 함수로 가봅시다.\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def create_examples_from_document(self, document: List[List[int]], doc_index: int):\n",
    "        \"\"\"Creates examples for a single document.\"\"\"\n",
    "        # 문장의 앞, 뒤에 [CLS], [SEP] token이 부착되기 때문에, 내가 지정한 size에서 2 만큼 빼줍니다.\n",
    "        # 예를 들어 128 token 만큼만 학습 가능한 model을 선언했다면, 학습 데이터로부터는 최대 126 token만 가져오게 됩니다.\n",
    "        max_num_tokens = self.block_size - self.tokenizer.num_special_tokens_to_add(pair=True)\n",
    "\n",
    "        # We *usually* want to fill up the entire sequence since we are padding\n",
    "        # to `block_size` anyways, so short sequences are generally wasted\n",
    "        # computation. However, we *sometimes*\n",
    "        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "        # sequences to minimize the mismatch between pretraining and fine-tuning.\n",
    "        # The `target_seq_length` is just a rough target however, whereas\n",
    "        # `block_size` is a hard limit.\n",
    "\n",
    "        # 여기가 재밌는 부분인데요!\n",
    "        # 위에서 설명했듯이, 학습 데이터는 126 token(128-2)을 채워서 만들어지는게 목적입니다.\n",
    "        # 하지만 나중에 BERT를 사용할 때, 126 token 이내의 짧은 문장을 테스트하는 경우도 분명 많을 것입니다 :-)\n",
    "        # 그래서 short_seq_probability 만큼의 데이터에서는 2-126 사이의 random 값으로 학습 데이터를 만들게 됩니다.\n",
    "        target_seq_length = max_num_tokens\n",
    "        if random.random() < self.short_seq_probability:\n",
    "            target_seq_length = random.randint(2, max_num_tokens)\n",
    "\n",
    "        current_chunk = []  # a buffer stored current working segments\n",
    "        current_length = 0\n",
    "        i = 0\n",
    "\n",
    "        # 데이터 구축의 단위는 document 입니다\n",
    "        # 이 때, 무조건 문장_1[SEP]문장_2 이렇게 만들어지는 것이 아니라,\n",
    "        # 126 token을 꽉 채울 수 있게 문장_1+문장_2[SEP]문장_3+문장_4 형태로 만들어질 수 있습니다.\n",
    "        while i < len(document):\n",
    "            segment = document[i]\n",
    "            current_chunk.append(segment)\n",
    "            current_length += len(segment)\n",
    "            if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "                if current_chunk:\n",
    "                    # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                    # (first) sentence.\n",
    "                    a_end = 1\n",
    "                    # 여기서 문장_1+문장_2 가 이루어졌을 때, 길이를 random하게 짤라버립니다 :-)\n",
    "                    if len(current_chunk) >= 2:\n",
    "                        a_end = random.randint(1, len(current_chunk) - 1)\n",
    "                    tokens_a = []\n",
    "                    for j in range(a_end):\n",
    "                        tokens_a.extend(current_chunk[j])\n",
    "                    # 이제 [SEP] 뒷 부분인 segmentB를 살펴볼까요?\n",
    "                    tokens_b = []\n",
    "                    # 50%의 확률로 랜덤하게 다른 문장을 선택하거나, 다음 문장을 학습데이터로 만듭니다.\n",
    "                    if len(current_chunk) == 1 or random.random() < self.nsp_probability:\n",
    "                        is_random_next = True\n",
    "                        target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "                        # This should rarely go for more than one iteration for large\n",
    "                        # corpora. However, just to be careful, we try to make sure that\n",
    "                        # the random document is not the same as the document\n",
    "                        # we're processing.\n",
    "                        for _ in range(10):\n",
    "                            random_document_index = random.randint(0, len(self.documents) - 1)\n",
    "                            if random_document_index != doc_index:\n",
    "                                break\n",
    "                        # 여기서 랜덤하게 선택합니다 :-)\n",
    "                        random_document = self.documents[random_document_index]\n",
    "                        random_start = random.randint(0, len(random_document) - 1)\n",
    "                        for j in range(random_start, len(random_document)):\n",
    "                            tokens_b.extend(random_document[j])\n",
    "                            if len(tokens_b) >= target_b_length:\n",
    "                                break\n",
    "                        # We didn't actually use these segments so we \"put them back\" so\n",
    "                        # they don't go to waste.\n",
    "                        num_unused_segments = len(current_chunk) - a_end\n",
    "                        i -= num_unused_segments\n",
    "                    # Actual next\n",
    "                    else:\n",
    "                        is_random_next = False\n",
    "                        for j in range(a_end, len(current_chunk)):\n",
    "                            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                    # 이제 126 token을 넘는다면 truncation을 해야합니다.\n",
    "                    # 이 때, 126 token 이내로 들어온다면 행위를 멈추고,\n",
    "                    # 만약 126 token을 넘는다면, segmentA와 segmentB에서 랜덤하게 하나씩 제거합니다.\n",
    "                    def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n",
    "                        \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "                        while True:\n",
    "                            total_length = len(tokens_a) + len(tokens_b)\n",
    "                            if total_length <= max_num_tokens:\n",
    "                                break\n",
    "                            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "                            assert len(trunc_tokens) >= 1\n",
    "                            # We want to sometimes truncate from the front and sometimes from the\n",
    "                            # back to add more randomness and avoid biases.\n",
    "                            if random.random() < 0.5:\n",
    "                                del trunc_tokens[0]\n",
    "                            else:\n",
    "                                trunc_tokens.pop()\n",
    "\n",
    "                    truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n",
    "\n",
    "                    assert len(tokens_a) >= 1\n",
    "                    assert len(tokens_b) >= 1\n",
    "\n",
    "                    # add special tokens\n",
    "                    input_ids = self.tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)\n",
    "                    # add token type ids, 0 for sentence a, 1 for sentence b\n",
    "                    token_type_ids = self.tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)\n",
    "                    \n",
    "                    # 드디어 아래 항목에 대한 데이터셋이 만들어졌습니다! :-)\n",
    "                    # 즉, segmentA[SEP]segmentB, [0, 0, .., 0, 1, 1, ..., 1], NSP 데이터가 만들어진 것입니다 :-)\n",
    "                    # 그럼 다음은.. 이 데이터에 [MASK] 를 씌워야겠죠?\n",
    "                    example = {\n",
    "                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                        \"next_sentence_label\": torch.tensor(1 if is_random_next else 0, dtype=torch.long),\n",
    "                    }\n",
    "\n",
    "                    self.examples.append(example)\n",
    "\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "wp_tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,  \n",
    "    strip_accents=False,    \n",
    "    lowercase=True,\n",
    ")\n",
    "wp_tokenizer.train(\n",
    "    files=\"/opt/ml/wine/data/text_data.txt\",\n",
    "    vocab_size=40000,  \n",
    "    min_frequency=3,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    wordpieces_prefix=\"##\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_tokenizer.save_model(\"/opt/ml/wine/code/models\", \"review_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertTokenizerFast' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e615319dec75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tokenizer = BertTokenizerFast(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/opt/ml/wine/code//text/models/review_tokenizer-vocab.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertTokenizerFast' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = BertTokenizerFast(\n",
    "    vocab_file='/opt/ml/wine/code//text/models/review_tokenizer-vocab.txt',\n",
    "    max_len=128,\n",
    "    do_lower_case=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = TextDatasetForNextSentencePrediction(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='/opt/ml/wine/data/text_data.txt',\n",
    "    block_size=128,\n",
    "    overwrite_cache=False,\n",
    "    short_seq_probability=0.2,\n",
    "    nsp_probability=0.5,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='/opt/ml/wine/code/models',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1000,\n",
    "    per_gpu_train_batch_size=32,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # wiki 전체 데이터로 학습 시, 1 epoch에 9시간 정도 소요됩니다!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('/opt/ml/wine/code/models/model_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /opt/ml/wine/code/text/models/model_output were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('/opt/ml/wine/code/text/models/model_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(df, model):\n",
    "    review_vectors = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in tqdm(range(len(df))):\n",
    "            reviews = df['text'][i].split('.')\n",
    "            id = df['wine_id'][i]\n",
    "            review_vector = []\n",
    "\n",
    "            for text in tqdm(reviews):\n",
    "                try:\n",
    "                    encoded_input = tokenizer.encode_plus(\n",
    "                        text, \n",
    "                        truncation = True,\n",
    "                        add_special_tokens=True, \n",
    "                        return_tensors='pt')\n",
    "                    model_output = model(**encoded_input)\n",
    "                    embeddings = model_output.last_hidden_state\n",
    "                    sentence_embedding = torch.mean(embeddings[0], dim=0)\n",
    "                    review_vector.append(sentence_embedding)\n",
    "\n",
    "                    del embeddings\n",
    "                    del model_output\n",
    "                    del encoded_input\n",
    "                    gc.collect()\n",
    "                    \n",
    "                except: 1\n",
    "\n",
    "            del review_vector\n",
    "            del sentence_embedding\n",
    "            gc.collect()\n",
    "            mean_vector = torch.mean(torch.stack(review_vector), dim=0).numpy()\n",
    "            review_vectors[id] = mean_vector\n",
    "        \n",
    "    return review_vectors\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:08<00:00,  2.53it/s]\n",
      "100%|██████████| 31/31 [00:11<00:00,  2.59it/s]s/it]\n",
      "100%|██████████| 473/473 [03:20<00:00,  2.36it/s]it]\n",
      "100%|██████████| 964/964 [06:26<00:00,  2.49it/s]/it]\n",
      "100%|██████████| 860/860 [05:45<00:00,  2.49it/s]5s/it]\n",
      "100%|██████████| 41/41 [00:16<00:00,  2.47it/s].84s/it]\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.64it/s].14s/it]\n",
      "100%|██████████| 22/22 [00:08<00:00,  2.59it/s]32s/it] \n",
      "100%|██████████| 1229/1229 [08:13<00:00,  2.49it/s]t] \n",
      "100%|██████████| 18/18 [00:07<00:00,  2.56it/s].46s/it]\n",
      "100%|██████████| 248/248 [01:38<00:00,  2.52it/s]74s/it]\n",
      "100%|██████████| 910/910 [06:10<00:00,  2.46it/s]4s/it] \n",
      "100%|██████████| 45/45 [00:18<00:00,  2.46it/s]5.84s/it]\n",
      "100%|██████████| 32/32 [00:12<00:00,  2.61it/s]9.03s/it]\n",
      "100%|██████████| 770/770 [05:09<00:00,  2.48it/s]2s/it] \n",
      "100%|██████████| 1331/1331 [08:57<00:00,  2.48it/s]s/it]\n",
      "100%|██████████| 741/741 [04:55<00:00,  2.51it/s]67s/it]\n",
      "100%|██████████| 113/113 [00:44<00:00,  2.53it/s]45s/it]\n",
      "100%|██████████| 1027/1027 [07:03<00:00,  2.43it/s]s/it]\n",
      "100%|██████████| 39/39 [00:15<00:00,  2.47it/s]5.67s/it]\n",
      "100%|██████████| 13/13 [00:05<00:00,  2.36it/s]197.65s/it]\n",
      "100%|██████████| 5555/5555 [14:07<00:00,  6.55it/s]98s/it]\n",
      "100%|██████████| 22/22 [00:00<00:00, 66.47it/s]352.41s/it]\n",
      "100%|██████████| 35/35 [00:00<00:00, 62.88it/s]246.76s/it]\n",
      "100%|██████████| 20/20 [00:00<00:00, 61.61it/s]172.89s/it]\n",
      "100%|██████████| 618/618 [00:11<00:00, 55.80it/s].11s/it] \n",
      "100%|██████████| 322/322 [00:05<00:00, 55.46it/s]10s/it] \n",
      "100%|██████████| 28/28 [00:00<00:00, 56.36it/s]3.41s/it]\n",
      "100%|██████████| 5291/5291 [01:35<00:00, 55.12it/s]s/it]\n",
      "100%|██████████| 1317/1317 [00:22<00:00, 58.43it/s]s/it]\n",
      "100%|██████████| 28/28 [00:00<00:00, 67.14it/s]8.75s/it]\n",
      "100%|██████████| 1027/1027 [00:14<00:00, 70.44it/s]s/it]\n",
      "100%|██████████| 35/35 [00:00<00:00, 66.93it/s]8.35s/it]\n",
      "100%|██████████| 555/555 [00:07<00:00, 69.57it/s]00s/it]\n",
      "100%|██████████| 693/693 [00:09<00:00, 69.70it/s]40s/it]\n",
      "100%|██████████| 1389/1389 [00:20<00:00, 69.44it/s]s/it]\n",
      "100%|██████████| 568/568 [00:15<00:00, 37.48it/s]13s/it]\n",
      "100%|██████████| 1091/1091 [00:14<00:00, 74.09it/s]s/it]\n",
      "100%|██████████| 1038/1038 [00:15<00:00, 67.52it/s]s/it]\n",
      "100%|██████████| 42/42 [00:00<00:00, 68.21it/s]5.47s/it]\n",
      "100%|██████████| 22/22 [00:00<00:00, 71.15it/s].01s/it] \n",
      "100%|██████████| 44/44 [00:00<00:00, 66.96it/s].80s/it]\n",
      "100%|██████████| 546/546 [00:07<00:00, 72.01it/s]6s/it]\n",
      "100%|██████████| 216/216 [00:03<00:00, 61.72it/s]4s/it]\n",
      "100%|██████████| 1653/1653 [00:33<00:00, 48.90it/s]/it]\n",
      "100%|██████████| 10/10 [00:00<00:00, 64.86it/s]3.94s/it]\n",
      "100%|██████████| 849/849 [00:45<00:00, 18.68it/s]0s/it] \n",
      "100%|██████████| 721/721 [00:28<00:00, 25.70it/s]50s/it]\n",
      "100%|██████████| 420/420 [00:08<00:00, 52.35it/s]77s/it]\n",
      "100%|██████████| 26/26 [00:00<00:00, 56.63it/s]8.35s/it]\n",
      "100%|██████████| 23/23 [00:00<00:00, 62.49it/s].98s/it] \n",
      "100%|██████████| 28/28 [00:00<00:00, 67.35it/s].20s/it]\n",
      "100%|██████████| 1010/1010 [00:19<00:00, 51.87it/s]/it]\n",
      "100%|██████████| 1447/1447 [00:26<00:00, 54.72it/s]/it]\n",
      "100%|██████████| 34/34 [00:00<00:00, 49.11it/s]5.24s/it]\n",
      "100%|██████████| 6211/6211 [01:50<00:00, 56.28it/s]/it] \n",
      "100%|██████████| 384/384 [00:07<00:00, 52.50it/s]73s/it]\n",
      "100%|██████████| 13/13 [00:00<00:00, 42.75it/s]0.71s/it]\n",
      "100%|██████████| 6990/6990 [01:59<00:00, 58.72it/s]s/it]\n",
      "100%|██████████| 41/41 [00:00<00:00, 68.97it/s]0.83s/it]\n",
      "100%|██████████| 27/27 [00:00<00:00, 78.23it/s]5.76s/it]\n",
      "100%|██████████| 547/547 [00:08<00:00, 67.16it/s]14s/it]\n",
      "100%|██████████| 16/16 [00:00<00:00, 54.62it/s]0.04s/it]\n",
      "100%|██████████| 302/302 [00:04<00:00, 65.36it/s]12s/it]\n",
      "100%|██████████| 130/130 [00:01<00:00, 75.07it/s]7s/it] \n",
      "100%|██████████| 12/12 [00:00<00:00, 71.74it/s].41s/it]\n",
      "100%|██████████| 23/23 [00:00<00:00, 64.66it/s].94s/it]\n",
      "100%|██████████| 694/694 [00:17<00:00, 39.90it/s]6s/it]\n",
      "100%|██████████| 32/32 [00:00<00:00, 51.39it/s].20s/it]\n",
      "100%|██████████| 377/377 [00:06<00:00, 56.88it/s]3s/it]\n",
      "100%|██████████| 1309/1309 [00:34<00:00, 37.62it/s]/it]\n",
      "100%|██████████| 3259/3259 [01:27<00:00, 37.31it/s]s/it]\n",
      "100%|██████████| 1053/1053 [00:18<00:00, 56.28it/s]s/it]\n",
      "100%|██████████| 15/15 [00:00<00:00, 57.63it/s]1.18s/it]\n",
      "100%|██████████| 1341/1341 [00:23<00:00, 56.93it/s]s/it]\n",
      "100%|██████████| 21/21 [00:00<00:00, 54.93it/s]2.41s/it]\n",
      "100%|██████████| 26/26 [00:00<00:00, 51.71it/s]5.80s/it]\n",
      "100%|██████████| 1987/1987 [00:36<00:00, 54.32it/s]/it] \n",
      "100%|██████████| 970/970 [00:17<00:00, 56.86it/s]83s/it]\n",
      "100%|██████████| 29/29 [00:00<00:00, 51.32it/s]8.30s/it]\n",
      "100%|██████████| 345/345 [00:06<00:00, 54.71it/s]8s/it] \n",
      "100%|██████████| 26/26 [00:00<00:00, 53.42it/s].98s/it]\n",
      "100%|██████████| 935/935 [00:16<00:00, 55.23it/s]3s/it]\n",
      "100%|██████████| 32/32 [00:00<00:00, 62.54it/s].56s/it]\n",
      "100%|██████████| 30/30 [00:00<00:00, 51.67it/s].55s/it]\n",
      "100%|██████████| 4959/4959 [01:29<00:00, 55.17it/s]/it]\n",
      "100%|██████████| 4864/4864 [01:12<00:00, 67.17it/s]s/it]\n",
      "100%|██████████| 18/18 [00:00<00:00, 64.96it/s]3.28s/it]\n",
      "100%|██████████| 27/27 [00:00<00:00, 66.12it/s]0.38s/it]\n",
      "100%|██████████| 367/367 [00:05<00:00, 65.44it/s]39s/it]\n",
      "100%|██████████| 886/886 [00:21<00:00, 41.92it/s]66s/it]\n",
      "100%|██████████| 32/32 [00:00<00:00, 75.32it/s]8.00s/it]\n",
      "100%|██████████| 43/43 [00:00<00:00, 79.90it/s].73s/it] \n",
      "100%|██████████| 2895/2895 [01:05<00:00, 44.27it/s]/it]\n",
      "100%|██████████| 726/726 [00:12<00:00, 57.18it/s]97s/it]\n",
      "100%|██████████| 24/24 [00:00<00:00, 55.35it/s]1.99s/it]\n",
      "100%|██████████| 618/618 [00:11<00:00, 56.00it/s]53s/it]\n",
      "100%|██████████| 1708/1708 [00:31<00:00, 54.34it/s]s/it]\n",
      "100%|██████████| 1046/1046 [00:19<00:00, 54.60it/s]s/it]\n",
      "100%|██████████| 250/250 [00:04<00:00, 53.30it/s]30s/it]\n",
      "100%|██████████| 23/23 [00:00<00:00, 50.48it/s]14.92s/it]\n",
      "100%|██████████| 23/23 [00:00<00:00, 59.79it/s]0.58s/it] \n",
      "100%|██████████| 576/576 [00:09<00:00, 58.35it/s]52s/it]\n",
      "100%|██████████| 29/29 [00:00<00:00, 55.09it/s]8.23s/it]\n",
      "100%|██████████| 41/41 [00:00<00:00, 50.73it/s]5.92s/it]\n",
      "100%|██████████| 24/24 [00:00<00:00, 53.87it/s]4.39s/it]\n",
      "100%|██████████| 5006/5006 [01:26<00:00, 57.85it/s]s/it]\n",
      "100%|██████████| 26/26 [00:00<00:00, 73.19it/s]28.21s/it]\n",
      "100%|██████████| 44/44 [00:00<00:00, 70.84it/s]19.85s/it]\n",
      "100%|██████████| 2429/2429 [00:33<00:00, 71.53it/s]9s/it]\n",
      "100%|██████████| 42/42 [00:00<00:00, 64.42it/s]20.05s/it]\n",
      "100%|██████████| 1311/1311 [00:24<00:00, 52.55it/s]3s/it]\n",
      "100%|██████████| 2747/2747 [01:15<00:00, 36.49it/s]5s/it]\n",
      "100%|██████████| 1388/1388 [00:43<00:00, 31.56it/s]0s/it]\n",
      "100%|██████████| 1233/1233 [00:22<00:00, 53.98it/s]6s/it]\n",
      "100%|██████████| 27/27 [00:00<00:00, 50.98it/s]33.15s/it]\n",
      "100%|██████████| 770/770 [00:13<00:00, 55.48it/s].36s/it]\n",
      "100%|██████████| 4611/4611 [01:24<00:00, 54.62it/s]2s/it]\n",
      "100%|██████████| 805/805 [00:14<00:00, 57.20it/s].69s/it]\n",
      "100%|██████████| 19/19 [00:00<00:00, 51.66it/s]32.01s/it]\n",
      "100%|██████████| 15/15 [00:00<00:00, 61.79it/s]22.52s/it]\n",
      "100%|██████████| 47/47 [00:00<00:00, 55.94it/s]15.84s/it]\n",
      "100%|██████████| 35/35 [00:00<00:00, 56.53it/s]1.34s/it] \n",
      "100%|██████████| 2815/2815 [00:51<00:00, 55.15it/s]s/it]\n",
      "100%|██████████| 21/21 [00:00<00:00, 60.23it/s]21.00s/it]\n",
      "100%|██████████| 832/832 [00:14<00:00, 56.40it/s].81s/it]\n",
      "100%|██████████| 1649/1649 [00:29<00:00, 55.65it/s]9s/it]\n",
      "100%|██████████| 1021/1021 [00:17<00:00, 56.74it/s]5s/it]\n",
      "100%|██████████| 1999/1999 [00:32<00:00, 61.05it/s]7s/it]\n",
      "100%|██████████| 807/807 [00:11<00:00, 67.76it/s].04s/it]\n",
      "100%|██████████| 28/28 [00:00<00:00, 74.86it/s]19.70s/it]\n",
      "100%|██████████| 1134/1134 [00:15<00:00, 71.18it/s]1s/it]\n",
      "100%|██████████| 652/652 [00:09<00:00, 71.48it/s].51s/it]\n",
      "100%|██████████| 23/23 [00:00<00:00, 75.75it/s]2.90s/it] \n",
      "100%|██████████| 358/358 [00:05<00:00, 71.20it/s]12s/it]\n",
      "100%|██████████| 41/41 [00:00<00:00, 66.25it/s]7.89s/it]\n",
      "100%|██████████| 960/960 [00:22<00:00, 42.05it/s]71s/it]\n",
      "100%|██████████| 2555/2555 [01:29<00:00, 28.47it/s]s/it]\n",
      "100%|██████████| 1218/1218 [00:26<00:00, 46.52it/s]2s/it]\n",
      "100%|██████████| 33/33 [00:00<00:00, 60.10it/s]32.02s/it]\n",
      "100%|██████████| 37/37 [00:00<00:00, 63.50it/s]22.58s/it]\n",
      "100%|██████████| 20/20 [00:00<00:00, 56.69it/s]15.98s/it]\n",
      "100%|██████████| 16235/16235 [04:47<00:00, 56.46it/s]it] \n",
      "100%|██████████| 1157/1157 [00:16<00:00, 71.46it/s]8s/it]\n",
      "100%|██████████| 133/133 [00:02<00:00, 65.33it/s].79s/it]\n",
      "100%|██████████| 513/513 [00:07<00:00, 69.35it/s].17s/it]\n",
      "100%|██████████| 23/23 [00:00<00:00, 68.53it/s]37.34s/it]\n",
      "100%|██████████| 904/904 [00:22<00:00, 40.90it/s].24s/it]\n",
      "100%|██████████| 650/650 [00:09<00:00, 67.61it/s].00s/it]\n",
      "100%|██████████| 1458/1458 [00:23<00:00, 62.99it/s]9s/it]\n",
      "100%|██████████| 132/132 [00:01<00:00, 68.79it/s].22s/it]\n",
      "100%|██████████| 17/17 [00:00<00:00, 71.51it/s]15.43s/it]\n",
      "100%|██████████| 381/381 [00:06<00:00, 59.23it/s]87s/it] \n",
      "100%|██████████| 38/38 [00:00<00:00, 62.85it/s]9.54s/it]\n",
      "100%|██████████| 1014/1014 [00:14<00:00, 69.52it/s]s/it]\n",
      "100%|██████████| 14/14 [00:00<00:00, 72.03it/s]9.18s/it]\n",
      "100%|██████████| 26/26 [00:00<00:00, 61.54it/s]6.49s/it]\n",
      "100%|██████████| 41/41 [00:00<00:00, 66.93it/s]4.67s/it]\n",
      "100%|██████████| 1546/1546 [00:21<00:00, 70.34it/s]s/it]\n",
      "100%|██████████| 418/418 [00:06<00:00, 66.67it/s]01s/it]\n",
      "100%|██████████| 28/28 [00:00<00:00, 72.31it/s]8.19s/it]\n",
      "100%|██████████| 19/19 [00:00<00:00, 73.59it/s]5.85s/it]\n",
      "100%|██████████| 68/68 [00:00<00:00, 70.28it/s]4.18s/it]\n",
      "100%|██████████| 22/22 [00:00<00:00, 71.79it/s]3.21s/it]\n",
      "100%|██████████| 700/700 [00:09<00:00, 70.51it/s]34s/it]\n",
      "100%|██████████| 692/692 [00:16<00:00, 42.64it/s]62s/it]\n",
      "100%|██████████| 30/30 [00:00<00:00, 68.93it/s]8.11s/it]\n",
      "100%|██████████| 1409/1409 [00:24<00:00, 57.29it/s]s/it]\n",
      "100%|██████████| 22/22 [00:00<00:00, 62.36it/s]1.44s/it]\n",
      "100%|██████████| 45/45 [00:00<00:00, 56.06it/s]8.12s/it]\n",
      "100%|██████████| 4740/4740 [01:10<00:00, 67.03it/s]s/it]\n",
      "100%|██████████| 1630/1630 [00:31<00:00, 51.13it/s]7s/it]\n",
      "100%|██████████| 7234/7234 [01:52<00:00, 64.19it/s]2s/it]\n",
      "100%|██████████| 23/23 [00:00<00:00, 68.27it/s]52.94s/it]\n",
      "100%|██████████| 313/313 [00:05<00:00, 57.15it/s].16s/it]\n",
      "100%|██████████| 1021/1021 [00:16<00:00, 62.63it/s]6s/it]\n",
      "100%|██████████| 402/402 [00:13<00:00, 28.93it/s].26s/it]\n",
      "100%|██████████| 414/414 [00:23<00:00, 17.33it/s].15s/it]\n",
      "100%|██████████| 30/30 [00:01<00:00, 19.65it/s]21.98s/it]\n",
      "100%|██████████| 44/44 [00:04<00:00, 10.80it/s]15.84s/it]\n",
      "100%|██████████| 29/29 [00:01<00:00, 22.06it/s]2.31s/it] \n",
      "100%|██████████| 28/28 [00:02<00:00, 12.28it/s]9.02s/it]\n",
      "100%|██████████| 1199/1199 [00:44<00:00, 27.10it/s]s/it]\n",
      "100%|██████████| 5468/5468 [01:41<00:00, 53.87it/s]7s/it]\n",
      "100%|██████████| 768/768 [00:14<00:00, 53.44it/s].18s/it]\n",
      "100%|██████████| 72/72 [00:01<00:00, 55.13it/s]34.54s/it]\n",
      "100%|██████████| 926/926 [00:16<00:00, 55.26it/s].57s/it]\n",
      "100%|██████████| 38/38 [00:00<00:00, 49.83it/s]22.23s/it]\n",
      "100%|██████████| 616/616 [00:11<00:00, 53.52it/s].79s/it]\n",
      "100%|██████████| 649/649 [00:12<00:00, 53.99it/s].51s/it]\n",
      "100%|██████████| 1308/1308 [00:24<00:00, 53.88it/s]s/it] \n",
      "100%|██████████| 34/34 [00:00<00:00, 65.65it/s]16.92s/it]\n",
      "100%|██████████| 566/566 [00:10<00:00, 55.55it/s]00s/it] \n",
      "100%|██████████| 107/107 [00:02<00:00, 51.92it/s]46s/it]\n",
      "100%|██████████| 22/22 [00:00<00:00, 48.30it/s]8.64s/it]\n",
      "100%|██████████| 231/231 [00:04<00:00, 54.03it/s]19s/it]\n",
      "100%|██████████| 34/34 [00:00<00:00, 51.88it/s]5.62s/it]\n",
      "100%|██████████| 589/589 [00:11<00:00, 51.54it/s]13s/it]\n",
      "100%|██████████| 104/104 [00:01<00:00, 53.96it/s]32s/it]\n",
      "100%|██████████| 24/24 [00:00<00:00, 50.92it/s]5.01s/it]\n",
      "100%|██████████| 884/884 [00:16<00:00, 53.11it/s]65s/it]\n",
      "100%|██████████| 428/428 [00:07<00:00, 55.37it/s]55s/it]\n",
      "100%|██████████| 1278/1278 [00:23<00:00, 53.56it/s]s/it]\n",
      "100%|██████████| 736/736 [00:12<00:00, 59.74it/s]48s/it]\n",
      "100%|██████████| 981/981 [00:13<00:00, 70.35it/s]44s/it]\n",
      "100%|██████████| 1349/1349 [00:20<00:00, 66.87it/s]s/it]\n",
      "100%|██████████| 41/41 [00:00<00:00, 64.52it/s]15.08s/it]\n",
      "100%|██████████| 502/502 [00:07<00:00, 69.37it/s]75s/it] \n",
      "100%|██████████| 1313/1313 [00:26<00:00, 50.36it/s]s/it]\n",
      "100%|██████████| 19/19 [00:00<00:00, 76.59it/s]14.61s/it]\n",
      "100%|██████████| 533/533 [00:08<00:00, 59.49it/s]30s/it] \n",
      "100%|██████████| 16/16 [00:00<00:00, 68.04it/s]9.90s/it]\n",
      "100%|██████████| 45/45 [00:01<00:00, 34.68it/s]7.00s/it]\n",
      "100%|██████████| 802/802 [00:12<00:00, 64.63it/s]29s/it]\n",
      "100%|██████████| 1389/1389 [00:39<00:00, 35.54it/s]s/it]\n",
      "100%|██████████| 46/46 [00:00<00:00, 70.52it/s]16.93s/it]\n",
      "100%|██████████| 27/27 [00:00<00:00, 70.41it/s]2.05s/it] \n",
      "100%|██████████| 25/25 [00:00<00:00, 48.56it/s]8.55s/it]\n",
      "100%|██████████| 821/821 [00:14<00:00, 56.91it/s]14s/it]\n",
      "100%|██████████| 1134/1134 [00:20<00:00, 55.17it/s]s/it]\n",
      "100%|██████████| 52/52 [00:00<00:00, 54.24it/s]2.21s/it]\n",
      "100%|██████████| 283/283 [00:05<00:00, 54.47it/s]83s/it]\n",
      "100%|██████████| 403/403 [00:07<00:00, 54.35it/s]74s/it]\n",
      "100%|██████████| 735/735 [00:13<00:00, 54.57it/s]65s/it]\n",
      "100%|██████████| 1238/1238 [00:22<00:00, 55.23it/s]s/it]\n",
      "100%|██████████| 959/959 [00:17<00:00, 55.66it/s]30s/it]\n",
      "100%|██████████| 965/965 [00:17<00:00, 55.51it/s].48s/it]\n",
      "100%|██████████| 32/32 [00:00<00:00, 58.19it/s]15.36s/it]\n",
      "100%|██████████| 1221/1221 [00:22<00:00, 55.22it/s]s/it] \n",
      "100%|██████████| 14/14 [00:00<00:00, 49.58it/s]14.28s/it]\n",
      "100%|██████████| 24/24 [00:00<00:00, 46.76it/s]0.08s/it] \n",
      "100%|██████████| 13/13 [00:00<00:00, 46.98it/s]7.21s/it]\n",
      "100%|██████████| 504/504 [00:08<00:00, 57.75it/s]13s/it]\n",
      "100%|██████████| 919/919 [00:14<00:00, 63.11it/s]21s/it]\n",
      "100%|██████████| 29/29 [00:00<00:00, 69.19it/s]8.72s/it]\n",
      "100%|██████████| 48/48 [00:00<00:00, 70.82it/s]6.23s/it]\n",
      "100%|██████████| 34/34 [00:00<00:00, 65.54it/s]4.57s/it]\n",
      "100%|██████████| 18/18 [00:00<00:00, 69.28it/s]3.35s/it]\n",
      "100%|██████████| 264/264 [00:03<00:00, 70.16it/s]43s/it]\n",
      "100%|██████████| 1153/1153 [00:16<00:00, 70.00it/s]s/it]\n",
      "100%|██████████| 1196/1196 [00:16<00:00, 72.28it/s]s/it]\n",
      "100%|██████████| 301/301 [00:04<00:00, 71.05it/s]81s/it]\n",
      "100%|██████████| 4970/4970 [02:18<00:00, 35.94it/s]s/it]\n",
      "100%|██████████| 830/830 [00:15<00:00, 53.55it/s].19s/it]\n",
      "100%|██████████| 977/977 [00:18<00:00, 53.52it/s].68s/it]\n",
      "100%|██████████| 24/24 [00:00<00:00, 64.25it/s]31.86s/it]\n",
      "100%|██████████| 628/628 [00:11<00:00, 55.33it/s].41s/it]\n",
      "100%|██████████| 1421/1421 [00:27<00:00, 52.08it/s]0s/it]\n",
      "100%|██████████| 815/815 [00:14<00:00, 55.88it/s].56s/it]\n",
      "100%|██████████| 392/392 [00:06<00:00, 57.38it/s].47s/it]\n",
      "100%|██████████| 35/35 [00:00<00:00, 50.16it/s]15.68s/it]\n",
      "100%|██████████| 19/19 [00:00<00:00, 44.06it/s]1.19s/it] \n",
      "100%|██████████| 34/34 [00:00<00:00, 60.28it/s]7.96s/it]\n",
      "100%|██████████| 559/559 [00:09<00:00, 56.40it/s]74s/it]\n",
      "100%|██████████| 767/767 [00:14<00:00, 53.84it/s]00s/it]\n",
      "100%|██████████| 36/36 [00:00<00:00, 55.80it/s]9.17s/it]\n",
      "100%|██████████| 649/649 [00:12<00:00, 53.00it/s]62s/it]\n",
      "100%|██████████| 39/39 [00:00<00:00, 59.22it/s]8.31s/it]\n",
      "100%|██████████| 22/22 [00:00<00:00, 55.93it/s]6.01s/it]\n",
      "100%|██████████| 30/30 [00:00<00:00, 54.51it/s]4.33s/it]\n",
      "100%|██████████| 28/28 [00:00<00:00, 53.72it/s]3.20s/it]\n",
      "100%|██████████| 393/393 [00:07<00:00, 50.94it/s]40s/it]\n",
      "100%|██████████| 22/22 [00:00<00:00, 59.14it/s]3.99s/it]\n",
      "100%|██████████| 36/36 [00:00<00:00, 50.66it/s]2.91s/it]\n",
      "100%|██████████| 1786/1786 [00:31<00:00, 55.83it/s]s/it]\n",
      "100%|██████████| 1506/1506 [00:27<00:00, 54.29it/s]s/it]\n",
      "100%|██████████| 535/535 [00:09<00:00, 54.16it/s].15s/it]\n",
      "100%|██████████| 10/10 [00:00<00:00, 56.88it/s]14.27s/it]\n",
      "100%|██████████| 20/20 [00:00<00:00, 53.37it/s]0.04s/it] \n",
      "100%|██████████| 942/942 [00:17<00:00, 53.89it/s]14s/it]\n",
      "100%|██████████| 1086/1086 [00:17<00:00, 61.98it/s]s/it]\n",
      "100%|██████████| 53/53 [00:00<00:00, 61.71it/s]2.43s/it]\n",
      "100%|██████████| 183/183 [00:02<00:00, 61.02it/s]96s/it]\n",
      "100%|██████████| 1544/1544 [00:22<00:00, 69.84it/s]s/it]\n",
      "100%|██████████| 1607/1607 [00:23<00:00, 67.17it/s]s/it]\n",
      "100%|██████████| 239/239 [00:03<00:00, 67.79it/s].34s/it]\n",
      "100%|██████████| 18/18 [00:02<00:00,  6.26it/s]1.80s/it] \n",
      "100%|██████████| 31/31 [00:04<00:00,  6.68it/s]9.12s/it]\n",
      "100%|██████████| 788/788 [00:13<00:00, 57.09it/s]78s/it]\n",
      "100%|██████████| 32/32 [00:00<00:00, 80.30it/s]9.59s/it]\n",
      "100%|██████████| 1110/1110 [00:17<00:00, 64.49it/s]s/it]\n",
      "100%|██████████| 1236/1236 [01:01<00:00, 20.21it/s]s/it]\n",
      "100%|██████████| 573/573 [00:23<00:00, 24.62it/s].31s/it]\n",
      "100%|██████████| 25/25 [00:00<00:00, 46.23it/s]24.70s/it]\n",
      "100%|██████████| 29/29 [00:00<00:00, 61.54it/s]17.46s/it]\n",
      "100%|██████████| 209/209 [00:03<00:00, 53.71it/s]36s/it] \n",
      "100%|██████████| 102/102 [00:01<00:00, 56.54it/s]82s/it]\n",
      "100%|██████████| 640/640 [00:11<00:00, 55.51it/s]42s/it]\n",
      "100%|██████████| 56/56 [00:01<00:00, 53.51it/s]8.65s/it]\n",
      "100%|██████████| 37/37 [00:00<00:00, 56.18it/s]6.37s/it]\n",
      "100%|██████████| 25/25 [00:00<00:00, 49.39it/s]4.66s/it]\n",
      "100%|██████████| 39/39 [00:00<00:00, 50.51it/s]3.42s/it]\n",
      "100%|██████████| 408/408 [00:07<00:00, 56.01it/s]62s/it]\n",
      "100%|██████████| 24/24 [00:00<00:00, 55.94it/s]4.02s/it]\n",
      "100%|██████████| 3897/3897 [01:10<00:00, 54.97it/s]s/it]\n",
      "100%|██████████| 50/50 [00:00<00:00, 50.26it/s]23.33s/it]\n",
      "100%|██████████| 385/385 [00:06<00:00, 55.40it/s].63s/it]\n",
      "100%|██████████| 509/509 [00:08<00:00, 57.43it/s]73s/it] \n",
      "100%|██████████| 5284/5284 [01:34<00:00, 55.75it/s]s/it]\n",
      "100%|██████████| 45/45 [00:00<00:00, 47.99it/s]37.03s/it]\n",
      "100%|██████████| 538/538 [00:10<00:00, 53.03it/s].21s/it]\n",
      "100%|██████████| 21/21 [00:00<00:00, 50.12it/s]21.39s/it]\n",
      "100%|██████████| 441/441 [00:07<00:00, 55.66it/s].10s/it]\n",
      "100%|██████████| 30/30 [00:00<00:00, 51.07it/s]2.95s/it] \n",
      "100%|██████████| 28/28 [00:00<00:00, 69.02it/s]9.24s/it]\n",
      "100%|██████████| 29/29 [00:00<00:00, 49.76it/s]6.59s/it]\n",
      "100%|██████████| 232/232 [00:03<00:00, 63.18it/s]79s/it]\n",
      "100%|██████████| 40/40 [00:00<00:00, 59.82it/s]4.46s/it]\n",
      "100%|██████████| 15/15 [00:00<00:00, 62.05it/s]3.32s/it]\n",
      "100%|██████████| 28/28 [00:00<00:00, 46.82it/s]2.40s/it]\n",
      "100%|██████████| 973/973 [00:18<00:00, 51.88it/s]86s/it]\n",
      "100%|██████████| 38/38 [00:00<00:00, 38.45it/s]6.93s/it]\n",
      "100%|██████████| 17/17 [00:00<00:00, 58.37it/s]5.15s/it]\n",
      "100%|██████████| 37/37 [00:00<00:00, 52.78it/s]3.69s/it]\n",
      "100%|██████████| 881/881 [00:14<00:00, 59.07it/s]80s/it]\n",
      "100%|██████████| 209/209 [00:03<00:00, 56.91it/s]43s/it]\n",
      "100%|██████████| 342/342 [00:06<00:00, 56.44it/s]61s/it]\n",
      "100%|██████████| 1033/1033 [00:18<00:00, 55.61it/s]s/it]\n",
      "100%|██████████| 27/27 [00:00<00:00, 67.77it/s]9.60s/it]\n",
      "100%|██████████| 25/25 [00:00<00:00, 67.37it/s]6.84s/it]\n",
      "100%|██████████| 340/340 [00:05<00:00, 65.00it/s]90s/it]\n",
      "100%|██████████| 157/157 [00:02<00:00, 68.60it/s]00s/it]\n",
      "100%|██████████| 26/26 [00:00<00:00, 65.75it/s]4.19s/it]\n",
      "100%|██████████| 21/21 [00:00<00:00, 64.37it/s]3.05s/it]\n",
      "100%|██████████| 36/36 [00:00<00:00, 66.42it/s]2.24s/it]\n",
      "100%|██████████| 49/49 [00:00<00:00, 68.20it/s]1.73s/it]\n",
      "100%|██████████| 21/21 [00:00<00:00, 72.16it/s]1.43s/it]\n",
      "100%|██████████| 37/37 [00:00<00:00, 69.39it/s].09s/it] \n",
      "100%|██████████| 66/66 [00:00<00:00, 72.15it/s].08it/s]\n",
      "100%|██████████| 1036/1036 [00:14<00:00, 69.79it/s]t/s]\n",
      "100%|██████████| 522/522 [00:07<00:00, 67.25it/s]10s/it]\n",
      "100%|██████████| 20/20 [00:00<00:00, 70.19it/s]5.90s/it]\n",
      "100%|██████████| 833/833 [00:11<00:00, 70.31it/s]22s/it]\n",
      "100%|██████████| 14/14 [00:00<00:00, 72.95it/s]6.51s/it]\n",
      "100%|██████████| 22/22 [00:00<00:00, 67.31it/s]4.62s/it]\n",
      "100%|██████████| 36/36 [00:00<00:00, 71.61it/s]3.33s/it]\n",
      "100%|██████████| 18/18 [00:00<00:00, 75.96it/s]2.48s/it]\n",
      "100%|██████████| 1358/1358 [00:26<00:00, 51.38it/s]s/it]\n",
      "100%|██████████| 1063/1063 [00:18<00:00, 56.19it/s]s/it]\n",
      "100%|██████████| 25/25 [00:00<00:00, 62.14it/s]2.12s/it]\n",
      "100%|██████████| 968/968 [00:14<00:00, 66.60it/s]60s/it]\n",
      "100%|██████████| 1331/1331 [00:20<00:00, 64.18it/s]s/it]\n",
      "100%|██████████| 894/894 [00:13<00:00, 64.96it/s]49s/it]\n",
      "100%|██████████| 1222/1222 [00:17<00:00, 70.52it/s]s/it]\n",
      "100%|██████████| 19/19 [00:00<00:00, 65.56it/s]14.70s/it]\n",
      "100%|██████████| 18/18 [00:00<00:00, 74.24it/s]0.38s/it] \n",
      "100%|██████████| 25/25 [00:00<00:00, 69.07it/s]7.34s/it]\n",
      "100%|██████████| 25/25 [00:00<00:00, 63.25it/s]5.25s/it]\n",
      "100%|██████████| 23/23 [00:00<00:00, 63.77it/s]3.79s/it]\n",
      "100%|██████████| 614/614 [00:08<00:00, 70.01it/s]77s/it]\n",
      "100%|██████████| 11/11 [00:00<00:00, 82.28it/s]4.57s/it]\n",
      "100%|██████████| 424/424 [00:06<00:00, 66.21it/s]24s/it]\n",
      "100%|██████████| 34/34 [00:00<00:00, 70.40it/s]4.19s/it]\n",
      "100%|██████████| 955/955 [00:13<00:00, 70.54it/s]08s/it]\n",
      "100%|██████████| 1033/1033 [00:23<00:00, 43.71it/s]s/it]\n",
      "100%|██████████| 157/157 [00:02<00:00, 60.15it/s]45s/it]\n",
      "100%|██████████| 35/35 [00:00<00:00, 66.00it/s]8.80s/it]\n",
      "100%|██████████| 633/633 [00:09<00:00, 66.96it/s]32s/it]\n",
      "100%|██████████| 1595/1595 [00:26<00:00, 59.19it/s]s/it]\n",
      "100%|██████████| 19/19 [00:00<00:00, 68.37it/s]3.17s/it]\n",
      "100%|██████████| 435/435 [00:06<00:00, 71.34it/s]30s/it]\n",
      "100%|██████████| 3329/3329 [00:47<00:00, 69.62it/s]s/it]\n",
      "100%|██████████| 4281/4281 [01:13<00:00, 58.58it/s]9s/it]\n",
      "100%|██████████| 1261/1261 [00:18<00:00, 67.40it/s]6s/it]\n",
      "100%|██████████| 496/496 [00:07<00:00, 68.13it/s].86s/it]\n",
      "100%|██████████| 45/45 [00:00<00:00, 68.65it/s]23.79s/it]\n",
      "100%|██████████| 16/16 [00:00<00:00, 61.54it/s]16.85s/it]\n",
      "100%|██████████| 5980/5980 [01:36<00:00, 61.67it/s]s/it] \n",
      "100%|██████████| 3201/3201 [00:46<00:00, 68.94it/s]1s/it]\n",
      "100%|██████████| 35/35 [00:00<00:00, 58.27it/s]40.12s/it]\n",
      "100%|██████████| 3345/3345 [00:58<00:00, 57.00it/s]7s/it]\n",
      "100%|██████████| 778/778 [00:12<00:00, 64.75it/s].39s/it]\n",
      "100%|██████████| 34/34 [00:00<00:00, 67.10it/s]29.78s/it]\n",
      "100%|██████████| 1037/1037 [00:15<00:00, 68.88it/s]0s/it]\n",
      "100%|██████████| 52/52 [00:00<00:00, 72.58it/s]19.22s/it]\n",
      "100%|██████████| 22/22 [00:00<00:00, 72.21it/s]3.67s/it] \n",
      "100%|██████████| 186/186 [00:02<00:00, 67.99it/s]66s/it]\n",
      "100%|██████████| 1867/1867 [00:27<00:00, 68.11it/s]s/it]\n",
      "100%|██████████| 20/20 [00:00<00:00, 55.78it/s]3.53s/it]\n",
      "100%|██████████| 828/828 [00:17<00:00, 46.25it/s]58s/it]\n",
      "100%|██████████| 22/22 [00:00<00:00, 60.96it/s]2.08s/it]\n",
      "100%|██████████| 427/427 [00:08<00:00, 52.29it/s]57s/it]\n",
      "100%|██████████| 769/769 [00:11<00:00, 65.69it/s]45s/it]\n",
      "100%|██████████| 3657/3657 [00:54<00:00, 66.57it/s]s/it]\n",
      "100%|██████████| 431/431 [00:06<00:00, 67.98it/s].08s/it]\n",
      "100%|██████████| 11/11 [00:00<00:00, 34.77it/s]18.06s/it]\n",
      "100%|██████████| 15/15 [00:00<00:00, 77.90it/s]2.74s/it] \n",
      "100%|██████████| 863/863 [00:12<00:00, 66.92it/s]98s/it]\n",
      "100%|██████████| 1440/1440 [00:27<00:00, 52.07it/s]s/it]\n",
      "100%|██████████| 27/27 [00:00<00:00, 64.78it/s]15.41s/it]\n",
      "100%|██████████| 769/769 [00:13<00:00, 56.03it/s]91s/it] \n",
      "100%|██████████| 924/924 [00:14<00:00, 65.39it/s]76s/it]\n",
      "100%|██████████| 2747/2747 [00:50<00:00, 54.10it/s]s/it]\n",
      "100%|██████████| 606/606 [00:11<00:00, 54.47it/s].96s/it]\n",
      "100%|██████████| 966/966 [00:18<00:00, 53.38it/s].12s/it]\n",
      "100%|██████████| 1665/1665 [00:25<00:00, 66.43it/s]1s/it]\n",
      "100%|██████████| 1104/1104 [00:16<00:00, 65.49it/s]8s/it]\n",
      "100%|██████████| 13/13 [00:00<00:00, 55.07it/s]19.89s/it]\n",
      "100%|██████████| 35/35 [00:00<00:00, 73.05it/s]13.99s/it]\n",
      "100%|██████████| 395/395 [00:05<00:00, 68.32it/s]94s/it] \n",
      "100%|██████████| 134/134 [00:01<00:00, 68.67it/s]69s/it]\n",
      "100%|██████████| 22/22 [00:00<00:00, 78.44it/s]6.67s/it]\n",
      "100%|██████████| 995/995 [00:21<00:00, 46.57it/s]76s/it]\n",
      "100%|██████████| 1346/1346 [00:21<00:00, 63.46it/s]s/it]\n",
      "100%|██████████| 543/543 [00:08<00:00, 63.32it/s]18s/it]\n",
      "100%|██████████| 45/45 [00:01<00:00, 34.48it/s]1.80s/it]\n",
      "100%|██████████| 824/824 [00:13<00:00, 62.37it/s]66s/it]\n",
      "100%|██████████| 14/14 [00:00<00:00, 56.78it/s]0.02s/it]\n",
      "100%|██████████| 1020/1020 [00:15<00:00, 65.16it/s]s/it]\n",
      "100%|██████████| 25/25 [00:00<00:00, 76.60it/s]9.66s/it]\n",
      "100%|██████████| 995/995 [00:14<00:00, 70.38it/s]86s/it]\n",
      "100%|██████████| 31/31 [00:00<00:00, 51.29it/s]9.05s/it]\n",
      "100%|██████████| 32/32 [00:00<00:00, 49.43it/s]6.52s/it]\n",
      "100%|██████████| 825/825 [00:11<00:00, 70.23it/s]76s/it]\n",
      "100%|██████████| 20/20 [00:00<00:00, 70.78it/s]6.86s/it]\n",
      "100%|██████████| 1069/1069 [00:15<00:00, 68.85it/s]s/it]\n",
      "100%|██████████| 664/664 [00:15<00:00, 42.25it/s]08s/it]\n",
      "100%|██████████| 27/27 [00:00<00:00, 55.38it/s]0.37s/it]\n",
      "100%|██████████| 17/17 [00:00<00:00, 68.55it/s]7.41s/it]\n",
      "100%|██████████| 17/17 [00:00<00:00, 60.82it/s]5.26s/it]\n",
      "100%|██████████| 762/762 [00:13<00:00, 57.25it/s]77s/it]\n",
      "100%|██████████| 26/26 [00:00<00:00, 69.54it/s]6.63s/it]\n",
      "100%|██████████| 38/38 [00:00<00:00, 73.11it/s]4.76s/it]\n",
      "100%|██████████| 287/287 [00:04<00:00, 67.57it/s]49s/it]\n",
      "100%|██████████| 877/877 [00:18<00:00, 46.66it/s]72s/it]\n",
      "100%|██████████| 389/389 [00:25<00:00, 15.14it/s]24s/it]\n",
      "100%|██████████| 16/16 [00:03<00:00,  5.00it/s]3.48s/it]\n",
      "100%|██████████| 15/15 [00:02<00:00,  7.08it/s]0.40s/it]\n",
      "100%|██████████| 258/258 [00:23<00:00, 11.03it/s]91s/it]\n",
      "100%|██████████| 27/27 [00:00<00:00, 51.83it/s]2.56s/it]\n",
      "100%|██████████| 2079/2079 [00:38<00:00, 54.01it/s]s/it]\n",
      "100%|██████████| 1317/1317 [00:24<00:00, 54.46it/s]2s/it]\n",
      "100%|██████████| 286/286 [00:05<00:00, 56.67it/s].73s/it]\n",
      "100%|██████████| 25/25 [00:00<00:00, 50.68it/s]15.33s/it]\n",
      "100%|██████████| 2181/2181 [00:38<00:00, 57.31it/s]s/it] \n",
      "100%|██████████| 1603/1603 [00:29<00:00, 53.53it/s]3s/it]\n",
      "100%|██████████| 2109/2109 [00:40<00:00, 52.68it/s]1s/it]\n",
      "100%|██████████| 28/28 [00:00<00:00, 52.96it/s]27.63s/it]\n",
      "100%|██████████| 952/952 [00:18<00:00, 51.91it/s].50s/it]\n",
      "100%|██████████| 13/13 [00:00<00:00, 52.02it/s]19.16s/it]\n",
      "100%|██████████| 42/42 [00:00<00:00, 60.63it/s]3.49s/it] \n",
      "100%|██████████| 1221/1221 [00:22<00:00, 53.79it/s]s/it]\n",
      "100%|██████████| 2884/2884 [00:51<00:00, 56.24it/s]s/it]\n",
      "100%|██████████| 28/28 [00:00<00:00, 71.69it/s]24.88s/it]\n",
      "100%|██████████| 269/269 [00:04<00:00, 57.48it/s].54s/it]\n",
      "100%|██████████| 1554/1554 [03:18<00:00,  7.84it/s]s/it] \n",
      "100%|██████████| 27/27 [00:01<00:00, 24.47it/s]69.07s/it]\n",
      "100%|██████████| 704/704 [00:56<00:00, 12.46it/s].68s/it]\n",
      "100%|██████████| 765/765 [00:16<00:00, 45.04it/s].02s/it]\n",
      "100%|██████████| 37/37 [00:00<00:00, 46.70it/s]40.81s/it]\n",
      "100%|██████████| 19/19 [00:00<00:00, 51.04it/s]28.81s/it]\n",
      "100%|██████████| 517/517 [00:19<00:00, 26.68it/s].28s/it]\n",
      "100%|██████████| 17/17 [00:07<00:00,  2.38it/s]20.01s/it]\n",
      "100%|██████████| 18/18 [00:04<00:00,  4.30it/s]16.15s/it]\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.78it/s]2.56s/it] \n",
      "100%|██████████| 6364/6364 [02:32<00:00, 41.67it/s]s/it]\n",
      "100%|██████████| 808/808 [01:12<00:00, 11.13it/s].40s/it]\n",
      "100%|██████████| 1612/1612 [01:12<00:00, 22.38it/s]7s/it]\n",
      "100%|██████████| 35/35 [00:02<00:00, 16.45it/s]62.54s/it]\n",
      "100%|██████████| 440/440 [00:32<00:00, 13.74it/s].42s/it]\n",
      "100%|██████████| 210/210 [00:13<00:00, 16.07it/s].70s/it]\n",
      "100%|██████████| 32/32 [00:00<00:00, 76.53it/s]32.41s/it]\n",
      "100%|██████████| 995/995 [00:26<00:00, 37.51it/s].82s/it]\n",
      "100%|██████████| 901/901 [00:32<00:00, 27.74it/s].93s/it]\n",
      "100%|██████████| 396/396 [00:06<00:00, 57.34it/s].50s/it]\n",
      "100%|██████████| 839/839 [00:17<00:00, 47.94it/s].62s/it]\n",
      "100%|██████████| 25/25 [00:00<00:00, 56.25it/s]19.69s/it]\n",
      "100%|██████████| 24/24 [00:00<00:00, 59.99it/s]3.92s/it] \n",
      "100%|██████████| 235/235 [00:03<00:00, 63.61it/s]86s/it]\n",
      "100%|██████████| 16/16 [00:00<00:00, 60.18it/s]8.01s/it]\n",
      "100%|██████████| 19/19 [00:00<00:00, 65.32it/s]5.69s/it]\n",
      "100%|██████████| 500/500 [00:08<00:00, 60.33it/s]07s/it]\n",
      "100%|██████████| 33/33 [00:00<00:00, 68.24it/s]5.34s/it]\n",
      "100%|██████████| 26/26 [00:00<00:00, 63.30it/s]3.88s/it]\n",
      "100%|██████████| 700/700 [00:19<00:00, 36.63it/s]84s/it]\n",
      "100%|██████████| 17/17 [00:00<00:00, 48.33it/s]7.72s/it]\n",
      "100%|██████████| 26/26 [00:00<00:00, 63.76it/s]5.51s/it]\n",
      "100%|██████████| 114/114 [00:14<00:00,  8.03it/s]98s/it]\n",
      "100%|██████████| 50/50 [00:00<00:00, 68.41it/s]7.05s/it]\n",
      "100%|██████████| 39/39 [00:00<00:00, 64.70it/s]5.15s/it]\n",
      "100%|██████████| 27/27 [00:00<00:00, 69.92it/s]3.79s/it]\n",
      "100%|██████████| 20/20 [00:00<00:00, 68.61it/s]2.77s/it]\n",
      "100%|██████████| 841/841 [00:19<00:00, 42.20it/s]03s/it]\n",
      "100%|██████████| 5028/5028 [20:17<00:00,  4.13it/s]s/it]\n",
      "100%|██████████| 27/27 [00:00<00:00, 51.77it/s] 370.50s/it]\n",
      "100%|██████████| 313/313 [00:05<00:00, 60.15it/s]59.51s/it]\n",
      "100%|██████████| 8/8 [00:00<00:00, 66.68it/s]6, 183.22s/it]\n",
      "100%|██████████| 33/33 [00:00<00:00, 49.09it/s]128.29s/it] \n",
      "100%|██████████| 1028/1028 [00:19<00:00, 53.96it/s]1s/it] \n",
      "100%|██████████| 27/27 [00:00<00:00, 40.83it/s]68.72s/it]\n",
      "100%|██████████| 27/27 [00:00<00:00, 48.24it/s]48.31s/it]\n",
      "100%|██████████| 27/27 [00:00<00:00, 40.64it/s]33.98s/it]\n",
      "100%|██████████| 1678/1678 [00:30<00:00, 54.66it/s]9s/it]\n",
      "100%|██████████| 29/29 [00:00<00:00, 54.48it/s]26.00s/it]\n",
      "100%|██████████| 572/572 [00:11<00:00, 47.97it/s].36s/it]\n",
      "100%|██████████| 957/957 [00:18<00:00, 52.31it/s].43s/it]\n",
      "100%|██████████| 23/23 [00:00<00:00, 51.10it/s]16.99s/it]\n",
      "100%|██████████| 1090/1090 [25:53<00:00,  1.42s/it]s/it] \n",
      "100%|██████████| 779/779 [01:57<00:00,  6.64it/s]74.38s/it]\n",
      "100%|██████████| 631/631 [04:03<00:00,  2.59it/s]67.28s/it]\n",
      "100%|██████████| 234/234 [01:33<00:00,  2.51it/s]30.07s/it]\n",
      "100%|██████████| 35/35 [00:11<00:00,  3.06it/s] 259.03s/it]\n",
      "100%|██████████| 556/556 [06:20<00:00,  1.46it/s]84.76s/it]\n",
      "100%|██████████| 635/635 [09:46<00:00,  1.08it/s]43.59s/it]\n",
      "100%|██████████| 662/662 [05:10<00:00,  2.13it/s]46.47s/it]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.47it/s] 335.70s/it]\n",
      "100%|██████████| 502/502 [02:54<00:00,  2.88it/s]37.42s/it]\n",
      "100%|██████████| 700/700 [04:46<00:00,  2.44it/s]18.43s/it]\n",
      "100%|██████████| 28/28 [00:27<00:00,  1.01it/s] 238.83s/it]\n",
      "100%|██████████| 1099/1099 [11:14<00:00,  1.63it/s].53s/it]\n",
      "100%|██████████| 17/17 [00:06<00:00,  2.60it/s] 325.09s/it]\n",
      "100%|██████████| 252/252 [01:24<00:00,  2.98it/s]29.53s/it]\n",
      "100%|██████████| 27/27 [00:11<00:00,  2.34it/s] 186.03s/it]\n",
      "100%|██████████| 21/21 [00:06<00:00,  3.10it/s]133.69s/it] \n",
      " 98%|█████████▊| 4842/4946 [31:04<00:40,  2.60it/s]1s/it] \n",
      "  2%|▏         | 514/26237 [5:13:24<261:24:24, 36.58s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (137) must match the size of tensor b (128) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6a2e34cab5d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#review_vectors = parallel_embedding(merged_reviews, model, 8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreview_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-d71d06398767>\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(df, model)\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     return_tensors='pt')\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0msentence_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m   1014\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"absolute\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (137) must match the size of tensor b (128) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "#review_vectors = parallel_embedding(merged_reviews, model, 8)\n",
    "review_vectors = get_embedding(merged_reviews, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
