{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple, List\n",
    "from transformers import BertConfig, BertForPreTraining, BertTokenizerFast\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "# from IPython.display import Image\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import normal_\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import re\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import os\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.utils import logging\n",
    "logger = logging.get_logger(__name__)\n",
    "from filelock import FileLock\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = pd.read_csv('/opt/ml/wine/data/review_df_total.csv',encoding = 'utf-8-sig').loc[:,['user_url','rating','text','wine_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/opt/ml/wine/code/data/feature_map/item2idx.json','r') as f:\n",
    "    item2idx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = review_df[review_df['text'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df['text'] = review_df['text'].apply(lambda x: x + '.' if x[-1] != '.' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_english_and_digits(text):\n",
    "    # Remove any characters that are not English alphabets, digits, periods, or commas at the end of sentences\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9\\s.,]', '', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_text(data):\n",
    "    return '\\n'.join(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df['text'] = review_df['text'].apply(keep_english_and_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df['wine_id'] = review_df['wine_url'].map(item2idx)\n",
    "review_df = review_df[review_df['wine_id'].isna()==False]\n",
    "review_df['wine_id'] = review_df['wine_id'].astype('int').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df = pd.read_csv('/opt/ml/wine/data/wine_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_df = wine_df.filter(like='_child')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = {}\n",
    "import ast\n",
    "def str2dict(x):\n",
    "    try: return ast.literal_eval(x)\n",
    "    except: return {}\n",
    "def get_keys(x):\n",
    "    return set(x.keys())\n",
    "\n",
    "for col in note_df.columns:\n",
    "    note_df.loc[:,col] = note_df.loc[:,col].apply(str2dict)\n",
    "    sub_note = set()\n",
    "    for i in tqdm(range(len(note_df))):\n",
    "        subs = get_keys(note_df[col][i])\n",
    "        sub_note = sub_note | subs\n",
    "    notes[col.replace('_child','')] = sub_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_with_note = review_df.drop(['rating','wine_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_note_in_review(text, notes_data):\n",
    "    text = text.lower()\n",
    "    result = []\n",
    "    for key in notes:\n",
    "        if any(word in text for word in notes[key]):\n",
    "            result.append(1)\n",
    "        else: result.append(0)\n",
    "    return result\n",
    "\n",
    "def marking_data(df, notes_data):\n",
    "    df.reset_index(inplace = True)\n",
    "    note_df = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        note_onehot = check_note_in_review(df.loc[i,'text'], notes_data)\n",
    "        note_df.append(note_onehot)\n",
    "    \n",
    "    note_df = pd.DataFrame(note_df, columns = notes_data.keys())\n",
    "    merged_df = pd.concat([df, note_df], axis=1)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def parallel_dataframe_2input(func, df, notes_data, num_cpu):\n",
    "\n",
    "    chunks = np.array_split(df, num_cpu)\n",
    "\n",
    "    print('Parallelizing with ' +str(num_cpu)+'cores')\n",
    "    with Parallel(n_jobs = num_cpu, backend=\"multiprocessing\") as parallel:\n",
    "        results = parallel(delayed(func)(chunks[i], notes_data) for i in range(num_cpu))\n",
    "\n",
    "    for i,data in enumerate(results):\n",
    "        if i == 0:\n",
    "            output = data\n",
    "        else:\n",
    "            output = output.reset_index(drop=True)\n",
    "            data = data.reset_index(drop=True)\n",
    "            output += pd.concat([output, data], axis=0)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.DataFrame(text_with_note.loc[:,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallelizing with 8cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 320766/928505 [01:22<03:35, 2821.02it/s]"
     ]
    }
   ],
   "source": [
    "note_marked_text = parallel_dataframe_2input(marking_data, text, notes, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe.comment_text\n",
    "        self.targets = self.data.list\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset=new_df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, num_labels)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
